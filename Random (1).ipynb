{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps6bNPfLCowX",
        "outputId": "12f556c3-c5ce-4c22-c405-e4b4911f9d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.850833767587285\n",
            "Confusion Matrix:\n",
            "[[2069  690]\n",
            " [ 455 4462]]\n",
            "PCA-transformed test data shape: (7676, 10)\n",
            "Accuracy of loaded model: 0.850833767587285\n",
            "Confusion Matrix of loaded model:\n",
            "[[2069  690]\n",
            " [ 455 4462]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import joblib\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Frauddata_pca.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where the target is NaN\n",
        "df = df.dropna(subset=['IsFraud'])\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(columns=['IsFraud'])\n",
        "y = df['IsFraud']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a SimpleImputer instance to handle missing values in features\n",
        "imputer = SimpleImputer(strategy='mean')  # You can use other strategies like 'median' or 'most_frequent'\n",
        "\n",
        "# Create a PCA instance\n",
        "pca = PCA(n_components=10)  # Adjust the number of components as needed\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a pipeline with SimpleImputer, PCA, and Random Forest\n",
        "pipeline = Pipeline(steps=[('imputer', imputer), ('pca', pca), ('rf', rf)])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Save the model to .pkl format\n",
        "pkl_filename = 'fraud_d1.pkl'\n",
        "joblib.dump(pipeline, pkl_filename)\n",
        "\n",
        "# Save the model and PCA components to .h5 format\n",
        "h5_filename = 'fraud_d1.h5'\n",
        "with h5py.File(h5_filename, 'w') as h5file:\n",
        "    # Save the PCA components\n",
        "    h5file.create_dataset('pca_components', data=pca.components_)\n",
        "    h5file.create_dataset('pca_explained_variance', data=pca.explained_variance_)\n",
        "    h5file.create_dataset('pca_mean', data=pca.mean_)\n",
        "    h5file.create_dataset('pca_variance_ratio', data=pca.explained_variance_ratio_)\n",
        "\n",
        "    # Save the Random Forest model using joblib.dump\n",
        "    joblib.dump(rf, '/content/drive/MyDrive/Colab Notebooks/random_forest1.pkl')\n",
        "\n",
        "    # Read the model bytes and save to HDF5 dataset\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/random_forest1.pkl', 'rb') as f:\n",
        "        model_bytes = f.read()\n",
        "\n",
        "    # Convert model bytes to a numpy array of type 'S1' (fixed-width ASCII strings)\n",
        "    model_bytes_np = np.frombuffer(model_bytes, dtype='S1')\n",
        "\n",
        "    # Create HDF5 dataset for model bytes\n",
        "    h5file.create_dataset('random_forest', data=model_bytes_np)\n",
        "\n",
        "# Verify PCA transformation\n",
        "X_test_pca = pca.transform(imputer.transform(X_test))\n",
        "print(f\"PCA-transformed test data shape: {X_test_pca.shape}\")\n",
        "\n",
        "# Load the model and test the loaded model\n",
        "with h5py.File(h5_filename, 'r') as h5file:\n",
        "    # Load PCA components\n",
        "    pca_components = h5file['pca_components'][:]\n",
        "    pca_explained_variance = h5file['pca_explained_variance'][:]\n",
        "    pca_mean = h5file['pca_mean'][:]\n",
        "    pca_variance_ratio = h5file['pca_variance_ratio'][:]\n",
        "\n",
        "    # Recreate PCA\n",
        "    pca_loaded = PCA(n_components=10)\n",
        "    pca_loaded.components_ = pca_components\n",
        "    pca_loaded.explained_variance_ = pca_explained_variance\n",
        "    pca_loaded.mean_ = pca_mean\n",
        "    pca_loaded.explained_variance_ratio_ = pca_variance_ratio\n",
        "\n",
        "    # Load Random Forest model\n",
        "    model_bytes_np = h5file['random_forest'][:]\n",
        "    model_bytes = model_bytes_np.tobytes()\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/random_forest1_loaded.pkl', 'wb') as f:\n",
        "        f.write(model_bytes)\n",
        "\n",
        "    rf_loaded = joblib.load('/content/drive/MyDrive/Colab Notebooks/random_forest1_loaded.pkl')\n",
        "\n",
        "    # Verify PCA transformation and model prediction\n",
        "    X_test_pca_loaded = pca_loaded.transform(imputer.transform(X_test))\n",
        "    y_pred_loaded = rf_loaded.predict(X_test_pca_loaded)\n",
        "\n",
        "    # Calculate accuracy of the loaded model\n",
        "    accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "    print(f\"Accuracy of loaded model: {accuracy_loaded}\")\n",
        "\n",
        "    # Generate confusion matrix of the loaded model\n",
        "    conf_matrix_loaded = confusion_matrix(y_test, y_pred_loaded)\n",
        "    print(f\"Confusion Matrix of loaded model:\\n{conf_matrix_loaded}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import joblib\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Frauddata.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where the target is NaN\n",
        "df = df.dropna(subset=['IsFraud'])\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(columns=['IsFraud'])\n",
        "y = df['IsFraud']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a SimpleImputer instance to handle missing values in features\n",
        "imputer = SimpleImputer(strategy='mean')  # You can use other strategies like 'median' or 'most_frequent'\n",
        "\n",
        "# Create a PCA instance\n",
        "pca = PCA(n_components=10)  # Adjust the number of components as needed\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a pipeline with SimpleImputer, PCA, and Random Forest\n",
        "pipeline = Pipeline(steps=[('imputer', imputer), ('pca', pca), ('rf', rf)])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Save the model to .pkl format\n",
        "pkl_filename = 'fraud_d1.pkl'\n",
        "joblib.dump(pipeline, pkl_filename)\n",
        "\n",
        "# Save the model and PCA components to .h5 format\n",
        "h5_filename = 'fraud_d1.h5'\n",
        "with h5py.File(h5_filename, 'w') as h5file:\n",
        "    # Save the PCA components\n",
        "    h5file.create_dataset('pca_components', data=pca.components_)\n",
        "    h5file.create_dataset('pca_explained_variance', data=pca.explained_variance_)\n",
        "    h5file.create_dataset('pca_mean', data=pca.mean_)\n",
        "    h5file.create_dataset('pca_variance_ratio', data=pca.explained_variance_ratio_)\n",
        "\n",
        "    # Save the Random Forest model using joblib.dump\n",
        "    joblib.dump(rf, 'random_forest1.pkl')\n",
        "\n",
        "    # Read the model bytes and save to HDF5 dataset\n",
        "    with open('random_forest1.pkl', 'rb') as f:\n",
        "        model_bytes = f.read()\n",
        "\n",
        "    # Convert model bytes to a numpy array of type 'S1' (fixed-width ASCII strings)\n",
        "    model_bytes_np = np.frombuffer(model_bytes, dtype='S1')\n",
        "\n",
        "    # Create HDF5 dataset for model bytes\n",
        "    h5file.create_dataset('random_forest', data=model_bytes_np)\n",
        "\n",
        "# Verify PCA transformation\n",
        "X_test_pca = pca.transform(imputer.transform(X_test))\n",
        "print(f\"PCA-transformed test data shape: {X_test_pca.shape}\")\n",
        "\n",
        "# Load the model and test the loaded model\n",
        "with h5py.File(h5_filename, 'r') as h5file:\n",
        "    # Load PCA components\n",
        "    pca_components = h5file['pca_components'][:]\n",
        "    pca_explained_variance = h5file['pca_explained_variance'][:]\n",
        "    pca_mean = h5file['pca_mean'][:]\n",
        "    pca_variance_ratio = h5file['pca_variance_ratio'][:]\n",
        "\n",
        "    # Recreate PCA\n",
        "    pca_loaded = PCA(n_components=10)\n",
        "    pca_loaded.components_ = pca_components\n",
        "    pca_loaded.explained_variance_ = pca_explained_variance\n",
        "    pca_loaded.mean_ = pca_mean\n",
        "    pca_loaded.explained_variance_ratio_ = pca_variance_ratio\n",
        "\n",
        "    # Load Random Forest model\n",
        "    model_bytes_np = h5file['random_forest'][:]\n",
        "    model_bytes = model_bytes_np.tobytes()\n",
        "\n",
        "    with open('random_forest1_loaded.pkl', 'wb') as f:\n",
        "        f.write(model_bytes)\n",
        "\n",
        "    rf_loaded = joblib.load('random_forest1_loaded.pkl')\n",
        "\n",
        "    # Verify PCA transformation and model prediction\n",
        "    X_test_pca_loaded = pca_loaded.transform(imputer.transform(X_test))\n",
        "    y_pred_loaded = rf_loaded.predict(X_test_pca_loaded)\n",
        "\n",
        "    # Calculate accuracy of the loaded model\n",
        "    accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "    print(f\"Accuracy of loaded model: {accuracy_loaded}\")\n",
        "\n",
        "    # Generate confusion matrix of the loaded model\n",
        "    conf_matrix_loaded = confusion_matrix(y_test, y_pred_loaded)\n",
        "    print(f\"Confusion Matrix of loaded model:\\n{conf_matrix_loaded}\")\n",
        "\n",
        "# Load the trained pipeline\n",
        "pipeline = joblib.load('fraud_d1.pkl')\n",
        "\n",
        "# Feature columns used in the original dataset\n",
        "feature_columns = [\n",
        "    'TransactionID', 'CardNumber', 'CardID', 'Amount', 'AvgTransactionAmount', 'Last1TransactionAmount',\n",
        "    'Last2TransactionAmount', 'Last3TransactionAmount', 'Last4TransactionAmount', 'Last5TransactionAmount',\n",
        "    'WrongPasswordAttempts', 'MultipleSwipes', 'HighValueTransactions', 'FestivalTime', 'OfferPeriod',\n",
        "    'TransactionCountLastHour', 'TransactionCountLastDay', 'TransactionCountLastWeek', 'DifferentBillingAddress',\n",
        "    'ExpiryYear'\n",
        "]\n",
        "\n",
        "# Get transaction details from the user\n",
        "print(\"Enter transaction details:\")\n",
        "transaction_details = {}\n",
        "for col in feature_columns:\n",
        "    transaction_details[col] = float(input(f\"{col}: \"))\n",
        "\n",
        "# Convert the transaction details into a DataFrame\n",
        "transaction_df = pd.DataFrame([transaction_details])\n",
        "\n",
        "# Ensure the DataFrame columns are in the correct order\n",
        "transaction_df = transaction_df[feature_columns]\n",
        "\n",
        "print(\"User input as DataFrame:\")\n",
        "print(transaction_df)\n",
        "\n",
        "# Extract individual components from the pipeline\n",
        "imputer = pipeline.named_steps['imputer']\n",
        "pca = pipeline.named_steps['pca']\n",
        "rf = pipeline.named_steps['rf']\n",
        "\n",
        "# Preprocess the user input using the components\n",
        "transaction_imputed = imputer.transform(transaction_df)\n",
        "transaction_pca = pca.transform(transaction_imputed)\n",
        "\n",
        "# Predict fraud for the new transaction\n",
        "is_fraud = rf.predict(transaction_pca)\n",
        "\n",
        "# Print the prediction result\n",
        "if is_fraud[0] == 1:\n",
        "    print(\"The transaction is predicted to be fraudulent.\")\n",
        "else:\n",
        "    print(\"The transaction is predicted to be not fraudulent.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz1aQ7cGLwbV",
        "outputId": "1e352c85-8f4d-4810-9503-34a14fc7828e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6307972902553414\n",
            "Confusion Matrix:\n",
            "[[1276 1483]\n",
            " [1351 3566]]\n",
            "PCA-transformed test data shape: (7676, 10)\n",
            "Accuracy of loaded model: 0.6307972902553414\n",
            "Confusion Matrix of loaded model:\n",
            "[[1276 1483]\n",
            " [1351 3566]]\n",
            "Enter transaction details:\n",
            "TransactionID: 5678738\n",
            "CardNumber: 67543000000000\n",
            "CardID: 8769\n",
            "Amount: 123\n",
            "AvgTransactionAmount: 123.5\n",
            "Last1TransactionAmount: 123\n",
            "Last2TransactionAmount: 123\n",
            "Last3TransactionAmount: 123\n",
            "Last4TransactionAmount: 123\n",
            "Last5TransactionAmount: 123\n",
            "WrongPasswordAttempts: 0\n",
            "MultipleSwipes: 0\n",
            "HighValueTransactions: 0\n",
            "FestivalTime: 0\n",
            "OfferPeriod: 0\n",
            "TransactionCountLastHour: 0\n",
            "TransactionCountLastDay: 1\n",
            "TransactionCountLastWeek: 1\n",
            "DifferentBillingAddress: 0\n",
            "ExpiryYear: 2029\n",
            "User input as DataFrame:\n",
            "   TransactionID    CardNumber  CardID  Amount  AvgTransactionAmount  \\\n",
            "0      5678738.0  6.754300e+13  8769.0   123.0                 123.5   \n",
            "\n",
            "   Last1TransactionAmount  Last2TransactionAmount  Last3TransactionAmount  \\\n",
            "0                   123.0                   123.0                   123.0   \n",
            "\n",
            "   Last4TransactionAmount  Last5TransactionAmount  WrongPasswordAttempts  \\\n",
            "0                   123.0                   123.0                    0.0   \n",
            "\n",
            "   MultipleSwipes  HighValueTransactions  FestivalTime  OfferPeriod  \\\n",
            "0             0.0                    0.0           0.0          0.0   \n",
            "\n",
            "   TransactionCountLastHour  TransactionCountLastDay  \\\n",
            "0                       0.0                      1.0   \n",
            "\n",
            "   TransactionCountLastWeek  DifferentBillingAddress  ExpiryYear  \n",
            "0                       1.0                      0.0      2029.0  \n",
            "The transaction is predicted to be not fraudulent.\n"
          ]
        }
      ]
    }
  ]
}